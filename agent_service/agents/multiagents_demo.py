from langgraph.types import Send
from typing import Annotated
from langchain_core.tools import tool
from langgraph.prebuilt import InjectedState
from langgraph.types import Command
from langgraph.prebuilt import create_react_agent
from langchain_community.chat_models import ChatOllama
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))
from agent_service.tools.config_load import ConfigLoader
from agent_service.memory import MultiAgentState
from langchain_openai import ChatOpenAI
from agent_service.tools import extract_last_json,image_recognition_tool, normalize_instruction, parse_vision_results,camera_tool,robot_state_tool,robot_shake_hand,robot_wave_hand
from langchain.memory import ConversationBufferMemory
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START,END, MessagesState
from langchain_core.messages import HumanMessage,AIMessage,SystemMessage
import asyncio
import json
from langgraph.graph.message import add_messages
from agent_service.config import get_llm
import ast
import base64
from typing import List, Dict
from PIL import Image
import io
import numpy as np
import requests
from langgraph.checkpoint.memory import MemorySaver
from typing_extensions import TypedDict

class State(TypedDict):
    messages: Annotated[list, add_messages]

memory_saver = MemorySaver()

def base64_to_model_image(base64_str: str, target_size=(224, 224)) -> np.ndarray:
    # è§£ç  base64 ä¸ºå­—èŠ‚æµ
    img_bytes = base64.b64decode(base64_str)
    img = Image.open(io.BytesIO(img_bytes)).convert("RGB")

    # è°ƒæ•´å¤§å°ä¸ºç›®æ ‡æ¨¡å‹è¾“å…¥å°ºå¯¸
    img = img.resize(target_size)

    # è½¬ä¸º numpy æ•°ç»„ï¼Œæ ¼å¼ä¸º (H, W, C)
    img_np = np.array(img, dtype=np.uint8)

    # è½¬æ¢ä¸ºæ¨¡å‹è¦æ±‚çš„æ ¼å¼ (3, 224, 224)
    img_np = np.transpose(img_np, (2, 0, 1))  # ä» (H, W, C) -> (C, H, W)

    return img_np

def safe_json(obj):
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: safe_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [safe_json(v) for v in obj]
    else:
        return obj
# ç¤ºä¾‹ï¼šå°† dict è½¬ä¸º LangChain æ¶ˆæ¯å¯¹è±¡
def convert_to_langchain_messages(messages):
    converted = []
    for msg in messages:
        if isinstance(msg, dict):
            role = msg["role"]
            content = msg["content"]
            if role == "user":
                converted.append(HumanMessage(content=content))
            elif role == "assistant":
                converted.append(AIMessage(content=content))
            else:
                # å¯é€‰ï¼šå¤„ç† system ç­‰å…¶ä»–è§’è‰²
                converted.append(SystemMessage(content=content))
        else:
            converted.append(msg)
    return converted


class VisionAgent:
    def __init__(self, name="vision_agent", vision_model = None):
        self.name = name
        self.llm = get_llm() #use vllm qwen3
        self.model = vision_model

    # origin_messages:åŸæœ¬çš„chat agentè¾“å…¥ state:chat agentè¾“å‡º
    def __call__(self,origin_messages:str,  state: dict) -> dict:
        messages = state.get("messages", [])

        # è·å–ä¼ å…¥æ¶ˆæ¯
        if not messages:
            response = HumanMessage(content="æœªæ£€æµ‹åˆ°ç”¨æˆ·æŒ‡ä»¤ã€‚")
            print(f"[{self.name}] æ²¡æœ‰æ¥æ”¶åˆ°æ¶ˆæ¯")
            return {"messages": messages + [response]}
        print(f"[{self.name}] æ¥æ”¶åˆ°æ¶ˆæ¯: {messages}")
        response = messages[-1].content

        # å°†responseè§£æä¸ºjsonæ ¼å¼
        result = extract_last_json(response)
        if result is None:
            print("æ— æ³•è§£æ JSON ç»“æœï¼")
            print("æ— æ³•è§£æ JSONï¼Œæ¨¡å‹è¿”å›å†…å®¹ä¸ºï¼š", messages)
            # print("é”™è¯¯ä¿¡æ¯ï¼š", e)
            obj = None
            ref = None
            pla = None
        else:
            print("jsonè§£æç»“æœ:", result)
            obj = result.get("object", "æœªçŸ¥")
            ref = result.get("reference", "æœªçŸ¥")
            pla = result.get("placement", "æœªçŸ¥")
            print("ç›®æ ‡ç‰©ä½“ï¼š", obj)
            print("å‚è€ƒç‰©ä½“ï¼š", ref)
            print("æ”¾ç½®ç›®æ ‡ï¼š", pla)

        # åˆ©ç”¨å…³é”®è¯è°ƒç”¨vision toolæ£€æµ‹

        """
        åˆ©ç”¨LLMæå–å¾—åˆ°çš„ç‰©ä½“å’Œå…³é”®è¯é‡‡ç”¨vision toolè¿›è¡Œç‰©ä½“æ£€æµ‹
        """

        if obj:
            print("[vision agent]ç›®æ ‡ï¼š",
                  normalize_instruction(obj) + normalize_instruction(ref) + normalize_instruction(pla))
            result = image_recognition_tool.invoke({
                "input": {
                    "image_path_or_url": r"/24T/yyy/wyq/llm_robot_control/agent_service/tools/images/1.jpg",
                    # æ˜¯å¦é‡‡ç”¨æœ¬åœ°æ•°æ®
                    "use_camera": False,  # æ˜¯å¦é‡‡ç”¨ç›¸æœºå®æ—¶è¾“å…¥
                    "instruction": normalize_instruction(obj) + normalize_instruction(ref) + normalize_instruction(pla),
                    "model_path": self.model
                }
            })
            if type(result) != str:
                result["message"] = origin_messages
                print(f"[å›¾åƒè¯†åˆ«ç»“æœ]ï¼š{result}")
                response = AIMessage(content=f"[å›¾åƒè¯†åˆ«ç»“æœ]ï¼š{result}")
            else:
                print("æœªè¯†åˆ«åˆ°å›¾åƒä¸­éœ€è¦æ£€æµ‹çš„ç‰©ä½“ã€‚")
                response = AIMessage(content="[å›¾åƒè¯†åˆ«ç»“æœ]ï¼šæœªè¯†åˆ«åˆ°å›¾åƒä¸­éœ€è¦æ£€æµ‹çš„ç‰©ä½“ã€‚")
        else:
            # æ­£å¸¸è°ƒç”¨æ¨¡å‹
            print("æœªè¯†åˆ«åˆ°å¯¹è¯ä¸­éœ€è¦æ£€æµ‹çš„ç‰©ä½“ã€‚")
            response = AIMessage(content="[å›¾åƒè¯†åˆ«ç»“æœ]ï¼šæœªè¯†åˆ«åˆ°å›¾åƒä¸­éœ€è¦æ£€æµ‹çš„ç‰©ä½“ã€‚")

        return response

class RobotAgent:
    def __init__(self, name="robot_agent", model_name="qwen:14b"):
        self.name = name

    def __call__(self, state: dict = None,use_vision_agent:bool = False, camera_idx: List[int] = [0,1,2,3]) -> dict:
        messages = state.get("messages", [])
        memory_dict = state.get("memory", {})
        my_memory = memory_dict.get(self.name, [])
        self.use_vision_agent = use_vision_agent
        self.camera_idx = camera_idx
        if self.use_vision_agent:
            vision_results = state.get("detected_positions", {})  # vision_agent æä¾›
            vision_results = parse_vision_results(vision_results)
            img = vision_results["img"]
            img = None
            # è°ƒç”¨ VLA è¾…åŠ©æ„å›¾è§£æï¼ˆå¯é€‰ï¼Œä½†æ¨èï¼‰
            vla_response = self.query_vla(img, messages[-1].content if messages else "")

            vla_response = HumanMessage(content=json.dumps(vla_response, ensure_ascii=False))
            new_memory_dict = {
                **memory_dict,
                # self.name: my_memory + messages + [llm_response]
                self.name: my_memory + messages + [vla_response]
            }
            return {
                "messages": messages + [vla_response],
                "memory": new_memory_dict,
                # "robot_action": parsed_response
            }

        else:
            img = state.get("image", None)
            if img is not None:
                for camera_idx in self.camera_idx:
                    if camera_idx in img: 
                        # camera_idx 0 -->cam_high ,1-->cam_low , 2--> cam_left_wrist, 3-->cam_right_wrist
                        if camera_idx == 0:
                            img_cam_high = img[camera_idx]
                        elif camera_idx == 1:
                            img_cam_low = img[camera_idx]
                        elif camera_idx == 2:
                            img_cam_left_wrist = img[camera_idx]
                        elif camera_idx == 3:
                            img_cam_right_wrist = img[camera_idx]
                state = state.get("state", None)
                if state is not None:
                    vla_response = self.query_vla_pi0(img_cam_high,img_cam_low,img_cam_left_wrist, img_cam_right_wrist , messages[-1].content if messages else "",state)
                    print(f"[RobotAgent] VLA å“åº”: {vla_response}")
                    vla_response = AIMessage(content="VLAæ‰§è¡Œå®Œæ¯•ï¼Œå·²ç”ŸæˆåŠ¨ä½œåºåˆ—")

                    new_memory_dict = {
                        **memory_dict,
                        self.name: my_memory + messages + [vla_response]
                    }
                    return {
                        "messages": [vla_response],
                        "memory": new_memory_dict
                    }
                else:
                    raise ValueError("æœªæä¾›æœºå™¨äººçŠ¶æ€ï¼Œè¯·æ£€æŸ¥Robotè¾“å…¥çŠ¶æ€ã€‚")
            else:
                raise ValueError("æœªæä¾›å›¾åƒæ•°æ®ï¼Œè¯·æ£€æŸ¥Robotè¾“å…¥çŠ¶æ€ã€‚")

    def query_vla_pi0(self, img_cam_high, img_cam_low, img_cam_left_wrist, img_cam_right_wrist, prompt: str,state:None) -> str:
        """
        è°ƒç”¨ VLA æ¨¡å—è¿›è¡Œè§†è§‰è¯­è¨€åˆ†æï¼Œè¾…åŠ©æ„å›¾æ¨ç†ã€‚
        è¿”å›ç»“æœåº”è¯¥æ˜¯å¯¹ä»»åŠ¡æ›´è¯¦ç»†çš„ç†è§£æˆ–è§£æä¿¡æ¯ã€‚
        """
        from openpi.training import config
        from openpi.policies import policy_config
        from openpi.shared import download
        import numpy as np
        import os
        os.environ["CUDA_VISIBLE_DEVICES"] = "7"
        os.environ["OPENPI_DATA_HOME"] = "/24T/yyy/szx/openpi/download_model/openpi"
        config = config.get_config("pi0_aloha_sim")
        checkpoint_dir = download.maybe_download("s3://openpi-assets/checkpoints/pi0_aloha_sim")
        policy = policy_config.create_trained_policy(config, checkpoint_dir)
        if state is None:
            state = np.ones((14,), dtype=np.float32)

        inputvla ={
            "state": state,
            "images": {
                "cam_high": base64_to_model_image(img_cam_high),
                "cam_low": base64_to_model_image(img_cam_low),
                "cam_left_wrist": base64_to_model_image(img_cam_left_wrist),
                "cam_right_wrist": base64_to_model_image(img_cam_right_wrist),
            },
            "prompt": prompt,
        }
        try:
            action_chunk = policy.infer(inputvla)["actions"]
            print(action_chunk.shape)
            action_chunk = {"action": action_chunk}
            action_chunk =safe_json(action_chunk)  # ç¡®ä¿è¿”å›çš„ JSON å¯åºåˆ—åŒ–
            return action_chunk
        except Exception as e:
            return f"VLAè°ƒç”¨å¼‚å¸¸: {str(e)}"

    def _build_prompt(self, messages, vision_results, vla_result):
        task = messages[-1].content if messages else ""
        instruction = (
            "ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººæ§åˆ¶agentã€‚è¯·æ ¹æ®ç”¨æˆ·çš„ä»»åŠ¡æè¿°ã€è§†è§‰æ¨¡å—æä¾›çš„ç‰©ä½“ä½ç½®ã€"
            "ä»¥åŠVLAæ¨¡å—æä¾›çš„æ¨ç†ç»“æœï¼Œè¾“å‡ºå¦‚ä¸‹æ ¼å¼çš„ JSON æ§åˆ¶æŒ‡ä»¤ï¼š\n\n"
            '{\n'
            '  "action": "pick/place",\n'
            '  "object": "<ç‰©ä½“å>",\n'
            '  "target_location": [x, y, z]\n'
            '}\n\n'
        )
        vision_info = f"è§†è§‰è¯†åˆ«ç»“æœï¼ˆvision_agentï¼‰:\n{json.dumps(vision_results, ensure_ascii=False)}"
        vla_info = f"\nVLAæ¨ç†ç»“æœ:\n{vla_result}"
        return instruction + f"\nç”¨æˆ·ä»»åŠ¡:\n{task}\n\n" + vision_info + vla_info

    def query_vla(self, img, user_instruction: str) -> str:
        """
        è°ƒç”¨ VLA æ¨¡å—è¿›è¡Œè§†è§‰è¯­è¨€åˆ†æï¼Œè¾…åŠ©æ„å›¾æ¨ç†ã€‚
        è¿”å›ç»“æœåº”è¯¥æ˜¯å¯¹ä»»åŠ¡æ›´è¯¦ç»†çš„ç†è§£æˆ–è§£æä¿¡æ¯ã€‚
        """
        import requests

        try:
            if img is not None:
                resp = requests.post(
                    self.vla_url,
                    json={"instruction": user_instruction}
                )
                if resp.status_code == 200:
                    return resp.json().get("analysis", "æ— æ¨ç†ç»“æœ")
                else:
                    return f"VLAé”™è¯¯: HTTP {resp.status_code}"
            else:
                resp = {"desk":[1,1,1], "basket":[1,1.1,1.1],"banana":[1,1.1,1.2]}
                return resp

        except Exception as e:
            return f"VLAè°ƒç”¨å¼‚å¸¸: {str(e)}"


class PlanAgent:
    def __init__(self, name="plan_agent", vision_agent=None, robot_agent=None,use_vision_agent:bool = False, camera_idx: List[int] = [0,1,2,3]):
        self.llm = get_llm() #use vllm qwen3
        self.name = name
        self.vision_agent = vision_agent
        self.robot_agent = robot_agent
        self.use_vision_agent = use_vision_agent
        self.camera_idx = camera_idx

    def __call__(self, state: dict) -> dict:
        messages = state.get("messages", [])
        memory_dict = state.get("memory", {})
        messages = convert_to_langchain_messages(messages)
        my_memory = memory_dict.get(self.name, [])
        print("---------plan agent---------")
        if messages:
            print(f"[{self.name}] æ¥æ”¶åˆ°æ¶ˆæ¯: {messages[-1].content}")
        full_messages = my_memory + messages 
        #[HumanMessage(content='å°†ç¯®å­é‡Œçš„é¦™è•‰æ‹¿èµ·å¹¶æ”¾ç½®åˆ°æ¡Œé¢ä¸Šã€‚', additional_kwargs={}, response_metadata={})]
        print(full_messages)

        if self.use_vision_agent:
            ### use vision agent to detect objects

            extraction_prompt = """è¯·ä»æŒ‡ä»¤ä¸­æå–ç›®æ ‡ç‰©ä½“ (object)ã€å‚è€ƒç‰©ä½“ (reference)ã€ä»¥åŠï¼ˆå¦‚æœ‰æ˜ç¡®æè¿°ï¼‰æ”¾ç½®ç‰©ä½“ (placement)ï¼Œä»¥JSONæ ¼å¼è¿”å›è‹±æ–‡ç»“æœï¼Œä¾‹å¦‚ï¼š
                    {"object": "cup", "reference": "red box", "placement": "table"}ï¼Œè‹¥æœ‰å¤šä¸ªç‰©ä½“ï¼Œè¯·ä½¿ç”¨æ•°ç»„è¡¨ç¤ºï¼Œä¾‹å¦‚ï¼š
                    {"object": ["cup", "pen"], "reference": ["box"], "placement": ["table"]}ï¼Œå¦‚æœæŸé¡¹ç¼ºå¤±ï¼Œè¯·å¡«å…¥ç©ºå­—ç¬¦ä¸²""ã€‚è¯·æ³¨æ„ï¼šJSON ä¸­æ‰€æœ‰åç§°éœ€ç¿»è¯‘ä¸ºè‹±æ–‡ã€‚æŒ‡ä»¤ï¼š""" + \
                                messages[-1].content
            full_messages.append(HumanMessage(content=extraction_prompt))
            print(f"--full_messages--: {full_messages}")
            response = self.llm.invoke(full_messages)
            print(f"[chat_agent] è§£æä»»åŠ¡æŒ‡ä»¤ä¸º: {response.content}")

            # å…ˆè°ƒç”¨VisionAgentæ£€æµ‹ç‰©ä½“
            vision_state = {"messages": [HumanMessage(content=response.content)], "memory": state.get("memory", {})}
            vision_output = self.vision_agent(messages[-1].content, vision_state)
            detected_positions = vision_output.content
            robot_messages = [HumanMessage(content=response.content)]
            img_dict ={}

        else:
            img_dict = camera_tool.invoke({"camera_idx":self.camera_idx})
            state =robot_state_tool.invoke({})
            detected_positions=None
            robot_messages =full_messages

        # æŠŠæ£€æµ‹ç»“æœæ”¾å…¥çŠ¶æ€ï¼Œè°ƒç”¨RobotAgentæ‰§è¡Œ
        robot_state = {
            "messages": robot_messages,
            # "memory": vision_output.get("memory", {}),
            "state" : state,
            "detected_positions": detected_positions,
            "image":img_dict
        }
        robot_output = self.robot_agent(robot_state,self.use_vision_agent,self.camera_idx)

        # # è¿”å›æœºå™¨äººæ‰§è¡Œç»“æœï¼Œæ›´æ–°memory
        print(f"[{self.name}] RobotAgent è¾“å‡º: {robot_output.get('messages', [])}")
        # print(f"[{self.name}] RobotAgent memory: {robot_output.get('memory', {})}")
        return {
            "messages": robot_output.get("messages", []),
            "memory": robot_output.get("memory", {}),
        }

class ChatAgent2:
    def __init__(self, name="chat_agent", model_name="qwen3:14b"):
        self.name = name
        self.llm = get_llm()  # use vllm qwen3

    def __call__(self, state: dict) -> dict:
        # print("ğŸ¤– chat_agent received:", state["messages"])
        response = self.llm.invoke(state["messages"])
        # print("ğŸ’¬ chat_agent response:", response.content)
        return {"messages": [response]}

class InteractionAgent:
    def __init__(self, name="interaction_agent"):
        self.name = name

    def __call__(self, state: dict) -> dict:
        messages = state.get("messages", [])
        user_input = ""

        if isinstance(messages, list):
            # æ‹¼æ¥æ‰€æœ‰ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            user_input = " ".join(m.content if hasattr(m, "content") else m.get("content", "") for m in messages)

        print(f"[{self.name}] æ¥æ”¶åˆ°ä»»åŠ¡æè¿°: {user_input}")
        
        # ç®€å•æ„å›¾è¯†åˆ«ï¼šä½ å¯ä»¥æ›¿æ¢ä¸ºæ›´å¤æ‚çš„æ¨¡å‹æˆ–å…³é”®è¯å­—å…¸
        if "æŒ¥æ‰‹" in user_input or "wave" in user_input:
            result = self._wave()
        elif "æ¡æ‰‹" in user_input or "shake" in user_input:
            result = self._shake()
        else:
            result = {"status": "ignored", "reason": "æœªè¯†åˆ«äº¤äº’æ„å›¾"}

        print(f"[{self.name}] äº¤äº’ç»“æœ: {result}")
        return {
            "messages": [
                AIMessage(content=str(result))
            ]
        }

    def _wave(self):
        try:
            response = requests.post("http://172.16.20.112:5003/wavehand",timeout=5)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            return {
                        "messages": [
                            AIMessage(content="æŒ¥æ‰‹æˆåŠŸï¼Œå·²å®Œæˆï¼"),
                            {"role": "system", "content": "ä»»åŠ¡å·²å®Œæˆï¼Œä¸éœ€è¦è¿›ä¸€æ­¥æ“ä½œã€‚"}
                        ]
                    }
            return {"status": "error", "action": "wave", "error": str(e)}

    def _shake(self):
        try:
            response = requests.post("http://172.16.20.112:5003/handshake",timeout=5)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            return {"status": "error", "action": "shake", "error": str(e)}  
            

def create_task_handoff_tool(
    *, agent_name: str, description: str | None = None
    ):
    name = f"transfer_to_{agent_name}"
    description = description or f"Ask {agent_name} for help."

    @tool(name, description=description)
    def handoff_tool(
        # this is populated by the supervisor LLM
        task_description: Annotated[str,"ç”¨æˆ·è¾“å…¥å†…å®¹"],
        # these parameters are ignored by the LLM
        state: Annotated[State, InjectedState],
    ) -> Command:
        print(f"******[{agent_name}] *************æ¥æ”¶åˆ°ä»»åŠ¡æè¿°: {task_description}")
        message = {"role": "user", "content": task_description}
        # agent_input = {**state, "messages": [message]}
        agent_input = {"messages":state["messages"] + [message]}

        return Command(goto=[Send(agent_name, agent_input)],graph=Command.PARENT)

    return handoff_tool


handoff_chat_agent = create_task_handoff_tool(
    agent_name="chat_agent",
    description="Assign task to a chat agent.",
)





handoff_plan_agent = create_task_handoff_tool(
    agent_name="plan_agent",
    description="Assign task to a plan agent.",
)

handoff_interaction_agent = create_task_handoff_tool(
    agent_name="interaction_agent",
    description="Assign task to a interaction agent.",
)

supervisor = create_react_agent(
    model=get_llm(),
    tools=[handoff_chat_agent,
           handoff_plan_agent,
           handoff_interaction_agent,
           # handoff_robot_agent
           ],
    prompt=(
        "ä½ æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“åè°ƒå™¨ï¼Œåªè´Ÿè´£åˆ†å‘ä»»åŠ¡ã€‚\n"
        "- chat_agent: ä¸“é—¨è´Ÿè´£èŠå¤©ç±»å¯¹è¯ï¼ˆå¦‚æ‰“æ‹›å‘¼ã€èŠå¤©ã€æé—®å›ç­”ã€è®°å¿†æå–ç­‰ï¼‰ï¼Œä½ æ‹¥æœ‰é•¿æœŸçš„è®°å¿†ï¼Œä¸æ¶‰åŠåŠ¨ä½œæˆ–ç‰©ä½“ã€‚\n"
        "- plan_agent: ä»…è´Ÿè´£æ¶‰åŠâ€œæ‰§è¡Œâ€ã€â€œæ‹¿èµ·â€ã€â€œæ”¾ç½®â€ç­‰å…·æœ‰æ˜ç¡®åŠ¨ä½œç›®æ ‡çš„æŒ‡ä»¤ç±»ä»»åŠ¡ã€‚\n"
        "- interaction_agent: è´Ÿè´£æœºå™¨äººæ§åˆ¶å’Œæ‰§è¡Œä¸€äº›â€œæŒ¥æ‰‹â€ã€â€œæ¡æ‰‹â€ç­‰ç®€å•äº¤äº’åŠ¨ä½œã€‚\n"
        "ä½ ä¸åº”è‡ªå·±å®Œæˆä»»åŠ¡ï¼Œåªåº”æ ¹æ®å¯¹è¯å†…å®¹é€‰æ‹©ä¸€ä¸ªå·¥å…·å‡½æ•°ï¼ˆå¦‚ transfer_to_chat_agentã€transfer_to_plan_agent æˆ– transfer_to_interaction_agentï¼‰æ¥è°ƒç”¨ä¸‹ä¸€ä¸ª agentã€‚\n"
        "å¦‚æœå¯¹è¯ä»…æ˜¯å¯’æš„ã€é—®å€™ã€æé—®ã€è®°å¿†ç›¸å…³å†…å®¹ï¼Œåº”ä½¿ç”¨ transfer_to_chat_agentã€‚\n"
        "å¦‚æœå¯¹è¯æ¶‰åŠæ‰§è¡Œä»»åŠ¡ã€ç§»åŠ¨ç‰©ä½“ã€æ“ä½œæœºå™¨äººï¼Œè¯·ä½¿ç”¨ transfer_to_plan_agentã€‚\n"
        "å¦‚æœå¯¹è¯æ¶‰åŠç®€å•çš„æœºå™¨äººäº¤äº’åŠ¨ä½œï¼ˆå¦‚æŒ¥æ‰‹ã€æ¡æ‰‹ç­‰ï¼‰ï¼Œè¯·ä½¿ç”¨ transfer_to_interaction_agentã€‚\n"
        "å¦‚æœä»»åŠ¡å·²ç»å®Œæˆï¼Œä¸éœ€è¦å†è°ƒç”¨å·¥å…·ï¼Œè¯·ä¸è¦è°ƒç”¨ä»»ä½•å·¥å…·ï¼Œç›´æ¥åœæ­¢ã€‚"
    ),
    # prompt=(
    #     "ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è°ƒåº¦å‘˜ã€‚\n"
    #     "æ ¹æ®ç”¨æˆ·è¾“å…¥åˆ¤æ–­è¯¥äº¤ç»™å“ªä¸ªæ™ºèƒ½ä½“æ¥å¤„ç†ã€‚\n"
    #     "- å¦‚æœæ˜¯æ—¥å¸¸å¯¹è¯ã€èŠå¤©ã€è®°å¿†ç±»å†…å®¹ï¼Œè¯·è°ƒç”¨ transfer_to_chat_agent å·¥å…·ã€‚\n"
    #     # "- å¦‚æœä½ è®¤ä¸ºä»»åŠ¡å·²ç»å®Œæˆï¼Œä¸éœ€è¦å†è°ƒç”¨å·¥å…·ï¼Œè¯·ä¸è¦è°ƒç”¨ä»»ä½•å·¥å…·ï¼Œç›´æ¥åœæ­¢ã€‚\n"
    #     "ä½ ä¸åº”è¯¥äº²è‡ªå›ç­”ï¼Œåªè´Ÿè´£åˆ†å‘ä»»åŠ¡ã€‚"
        
    # ),
    name="supervisor"
)

loader = ConfigLoader("../config/vision_config.yaml")
yolo_cfg = loader.get_model_config("yolov8s-worldv2")

yolo_path = yolo_cfg["path"]
chat_agent = ChatAgent2()
vision_agent = VisionAgent(name="vision_agent", vision_model = yolo_path)
robot_agent = RobotAgent()
plan_agent = PlanAgent(vision_agent=vision_agent, robot_agent=robot_agent)

interaction_agent = InteractionAgent(name="interaction_agent")

graph_builder = (
    StateGraph(State)
    .add_node("supervisor", supervisor)
    .add_node("chat_agent", chat_agent)
    .add_node("plan_agent", plan_agent)
    .add_node("interaction_agent", interaction_agent)

    .add_edge(START, "supervisor")
    .add_edge("chat_agent", "supervisor")
    # .add_edge("plan_agent", "supervisor")
    .add_edge("interaction_agent", "supervisor")
    .add_edge("plan_agent", END)
    # .add_edge("supervisor", END)
    # .compile(checkpointer=memory_saver)
)
runnable = graph_builder.compile(checkpointer=memory_saver)


# if __name__ == "__main__":
config1 = {"configurable": {"thread_id": "session123"}}

output1 = runnable.invoke(
    {"messages": [{"role": "user", "content": "Hi, my name is Villala."}]},
    config=config1
)
print("ğŸ’¬ è¾“å‡º:", output1["messages"][-1].content)

state2={"messages": [{"role": "user", "content": "What is my name?"}]}

output2 = runnable.invoke(state2,config=config1)

print("ğŸ’¬ è¾“å‡º:", output2["messages"][-1].content)
state4 = {"messages": [{"role": "user", "content": "æŒ¥æ‰‹"}]}

output4 = runnable.invoke(state4,config=config1)
print("ç¬¬å››æ¬¡è¾“å‡º:", output4["messages"][-1].content)
state3 = {"messages": [{"role": "user", "content": "æŠŠç¯®å­é‡Œçš„é¦™è•‰æ”¾æ¡Œä¸Š"}]}

output3 = runnable.invoke(state3,config=config1)
print("ç¬¬ä¸‰æ¬¡è¾“å‡º:", output3["messages"][-1].content)
